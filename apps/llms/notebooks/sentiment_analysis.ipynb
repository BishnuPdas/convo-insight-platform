{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ram/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ram/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ram/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(650000, 3)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.read_csv(\n",
    "#     '/home/ram/convo-insight-platform/local_folder/Assets/datasets/Recommended/amazon_review_for_sentiment_analysis/test.csv',\n",
    "#     names=['class_index', 'review_title', 'review_text'])\n",
    "# print(df.shape)\n",
    "# df['class_index'] = df['class_index'].map(\n",
    "#     {\n",
    "#         1: -1,\n",
    "#         2: -1,\n",
    "#         3: 0,\n",
    "#         4: 1,\n",
    "#         5: 1,\n",
    "#     })\n",
    "# df.to_csv('amazon_reviews_sentiment_3cls.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_index</th>\n",
       "      <th>review_title</th>\n",
       "      <th>review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>466072</th>\n",
       "      <td>0</td>\n",
       "      <td>Sweet but rushed</td>\n",
       "      <td>Book provided by publisher.This was a sweet re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461081</th>\n",
       "      <td>-1</td>\n",
       "      <td>Didn't last as long as it supposed to...</td>\n",
       "      <td>I bought that 10 inches clamp for the 250 watt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178529</th>\n",
       "      <td>0</td>\n",
       "      <td>Only OK for using on painted surfaces</td>\n",
       "      <td>This roll of tape arrived just as I was painti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522706</th>\n",
       "      <td>-1</td>\n",
       "      <td>NOT FOR TOTAL BEGINNERS!!</td>\n",
       "      <td>Two seconds in and even -I- was lost. She at n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384322</th>\n",
       "      <td>1</td>\n",
       "      <td>Simply the best book I've read this year</td>\n",
       "      <td>I would recommend this book to anyone as an ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270251</th>\n",
       "      <td>0</td>\n",
       "      <td>A Re-release with bonus tracks</td>\n",
       "      <td>I have the original F**k You CD, I noticed tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633717</th>\n",
       "      <td>-1</td>\n",
       "      <td>Illogical trash</td>\n",
       "      <td>This is undoubtedly the worst book I have read...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263174</th>\n",
       "      <td>-1</td>\n",
       "      <td>Loditech around head headset</td>\n",
       "      <td>The very long chord is cumbersome. The product...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267464</th>\n",
       "      <td>0</td>\n",
       "      <td>Wow you better be a soprano.</td>\n",
       "      <td>This book is awesome, it has almost all the so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180969</th>\n",
       "      <td>-1</td>\n",
       "      <td>Wish I could rate this less than \"1\"...</td>\n",
       "      <td>I could only stand for about 15 mins of this, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        class_index                              review_title  \\\n",
       "466072            0                          Sweet but rushed   \n",
       "461081           -1  Didn't last as long as it supposed to...   \n",
       "178529            0     Only OK for using on painted surfaces   \n",
       "522706           -1                 NOT FOR TOTAL BEGINNERS!!   \n",
       "384322            1  Simply the best book I've read this year   \n",
       "270251            0            A Re-release with bonus tracks   \n",
       "633717           -1                           Illogical trash   \n",
       "263174           -1              Loditech around head headset   \n",
       "267464            0              Wow you better be a soprano.   \n",
       "180969           -1   Wish I could rate this less than \"1\"...   \n",
       "\n",
       "                                              review_text  \n",
       "466072  Book provided by publisher.This was a sweet re...  \n",
       "461081  I bought that 10 inches clamp for the 250 watt...  \n",
       "178529  This roll of tape arrived just as I was painti...  \n",
       "522706  Two seconds in and even -I- was lost. She at n...  \n",
       "384322  I would recommend this book to anyone as an ab...  \n",
       "270251  I have the original F**k You CD, I noticed tha...  \n",
       "633717  This is undoubtedly the worst book I have read...  \n",
       "263174  The very long chord is cumbersome. The product...  \n",
       "267464  This book is awesome, it has almost all the so...  \n",
       "180969  I could only stand for about 15 mins of this, ...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_data(file_path):\n",
    "#     data = pd.read_csv(file_path)\n",
    "#     return data\n",
    "\n",
    "\n",
    "# def preprocess_text(text):\n",
    "#     # Tokenize\n",
    "#     tokens = word_tokenize(text.lower())\n",
    "\n",
    "#     # Remove stopwords and non-alphabetic tokens\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     tokens = [token for token in tokens if token.isalpha()\n",
    "#               and token not in stop_words]\n",
    "\n",
    "#     # Lemmatize\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "#     return tokens\n",
    "\n",
    "\n",
    "# # Load the data\n",
    "# data = load_data('amazon_reviews_sentiment_3cls.csv')\n",
    "\n",
    "# # Preprocess the text\n",
    "# data['processed_text'] = data['review_text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Load the data\n",
    "data = load_data('amazon_reviews_sentiment_3cls.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Remove stopwords and non-alphabetic tokens\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token.isalpha()\n",
    "              and token not in stop_words]\n",
    "\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_csv('amazon_reviews_sentiment_3cls.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['This is a fast read filled with unexpected humour and profound insights into the art of politics and policy. In brief, it is sly, wry, and wise.',\n",
       "       'I bought one of these chargers..the instructions say the lights stay on while the battery charges...true. The instructions doNT say the lights turn off when its done. Which is also true. 24 hours of charging and the lights stay on. I returned it thinking I had a bad unit.The new one did the same thing. I just kept it since it does charge...but the lights are useless since they seem to always stay on. It\\'s a \"backup\" charger for when I manage to drain all my AAs but I wouldn\\'t want this as my only charger.',\n",
       "       'I was excited to find a book ostensibly about Muslim feminism, but this volume did not live up to the expectations.One essay, among other things, describes the veil as potentially liberating. It doesn\\'t begin to explain how or why.Another, on Muslim women in Cape Town, claims that Muslim women there are separate but \"more than equal.\" Gee whiz, what a disappointment.I had expected and hoped for at least one Muslim feminist condemnation of gender apartheid. But there is not a single one in the book.I\\'m surprised it didn\\'t have an essay extolling the virtues of female genital mutilation.--Alyssa A. Lappen',\n",
       "       'I am a big JVC fan, but I do not like this model, I was suspiscious when I saw several units in the return section of the store. I bought one anyway (new) and must say I am not happy. The unit sends out clicks to the receiver once in a while, the transition between scenes is not always smooth,(like a little pause) and while it is still fairly new I can\\'t get any DVD,CD or even a DVD headcleaner to work. All I get is a \"incorrect disc\" message.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['review_text'].iloc[1:5].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['fast', 'read', 'filled', 'unexpected', 'humour', 'profound', 'insight', 'art', 'politics', 'policy', 'brief', 'sly', 'wry', 'wise']),\n",
       "       list(['bought', 'one', 'charger', 'instruction', 'say', 'light', 'stay', 'battery', 'charge', 'true', 'instruction', 'dont', 'say', 'light', 'turn', 'done', 'also', 'true', 'hour', 'charging', 'light', 'stay', 'returned', 'thinking', 'bad', 'new', 'one', 'thing', 'kept', 'since', 'charge', 'light', 'useless', 'since', 'seem', 'always', 'stay', 'backup', 'charger', 'manage', 'drain', 'aa', 'would', 'want', 'charger']),\n",
       "       list(['excited', 'find', 'book', 'ostensibly', 'muslim', 'feminism', 'volume', 'live', 'essay', 'among', 'thing', 'describes', 'veil', 'potentially', 'liberating', 'begin', 'explain', 'muslim', 'woman', 'cape', 'town', 'claim', 'muslim', 'woman', 'separate', 'equal', 'gee', 'whiz', 'expected', 'hoped', 'least', 'one', 'muslim', 'feminist', 'condemnation', 'gender', 'apartheid', 'single', 'one', 'surprised', 'essay', 'extolling', 'virtue', 'female', 'genital', 'alyssa', 'lappen']),\n",
       "       list(['big', 'jvc', 'fan', 'like', 'model', 'suspiscious', 'saw', 'several', 'unit', 'return', 'section', 'store', 'bought', 'one', 'anyway', 'new', 'must', 'say', 'happy', 'unit', 'sends', 'click', 'receiver', 'transition', 'scene', 'always', 'smooth', 'like', 'little', 'pause', 'still', 'fairly', 'new', 'ca', 'get', 'dvd', 'cd', 'even', 'dvd', 'headcleaner', 'work', 'get', 'incorrect', 'disc', 'message'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['processed_text'].iloc[1:5].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create Word2Vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word2vec_model(sentences, vector_size=100, window=5, min_count=1, workers=4):\n",
    "    model = Word2Vec(sentences=sentences, vector_size=vector_size,\n",
    "                     window=window, min_count=min_count, workers=workers)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = create_word2vec_model(data['processed_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['word2vec_model.joblib']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the Word2Vec model\n",
    "joblib.dump(word2vec_model, 'word2vec_model.joblib')\n",
    "# To load the models later\n",
    "# loaded_word2vec_model = joblib.load('word2vec_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to get document embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_embedding(tokens, model, vector_size=100):\n",
    "    embeddings = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if len(embeddings) > 0:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "\n",
    "# Create document embeddings\n",
    "data['embedding'] = data['processed_text'].apply(\n",
    "    lambda x: get_document_embedding(x, word2vec_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([ 1.85620219e-01,  1.03138947e+00, -8.74463916e-01, -5.38157940e-01,\n",
       "              -5.32384396e-01,  2.11045817e-01,  6.81746006e-01, -1.28940772e-02,\n",
       "               9.84175280e-02, -3.38817805e-01, -5.09591460e-01, -4.37132567e-01,\n",
       "               8.94572064e-02, -6.89263642e-01,  5.07834852e-01,  3.18552643e-01,\n",
       "              -1.03236139e+00,  7.18246102e-01,  2.38192171e-01, -6.33275390e-01,\n",
       "               1.48602474e+00,  9.53968823e-01, -8.72778773e-01,  5.00126183e-01,\n",
       "              -5.06225467e-01,  1.89735174e-01,  1.04790807e+00, -5.65235198e-01,\n",
       "              -5.41883528e-01, -4.84074146e-01, -7.63076097e-02, -4.46439803e-01,\n",
       "               1.39961451e-01,  1.01297334e-01, -1.24037609e-01,  1.10631120e+00,\n",
       "               3.26375753e-01,  1.00893986e+00, -1.05913424e+00, -6.06448114e-01,\n",
       "               8.32322001e-01,  1.12268639e+00, -2.36056998e-01,  1.62508368e-01,\n",
       "              -3.01759154e-01,  6.41363263e-01, -1.04996645e+00, -2.42228255e-01,\n",
       "               9.56590666e-05, -1.29502130e+00, -6.01950347e-01, -8.60421658e-02,\n",
       "               7.77425617e-02, -7.77639985e-01, -4.55901563e-01,  5.04482985e-02,\n",
       "               1.35052955e+00, -5.07577479e-01,  4.92036402e-01,  1.19598317e+00,\n",
       "              -1.62562460e-01,  1.11091316e-01, -8.60812485e-01,  5.00900224e-02,\n",
       "               2.95072615e-01,  2.00448364e-01, -6.20067477e-01, -7.72421241e-01,\n",
       "              -7.01399505e-01, -4.79510128e-01, -1.03532307e-01, -1.73638184e-02,\n",
       "              -3.35134685e-01, -7.08892822e-01, -1.38523370e-01,  7.01331943e-02,\n",
       "              -7.19473362e-01, -3.86763290e-02, -1.01826787e+00, -1.09905526e-01,\n",
       "              -2.18309000e-01, -2.72562802e-01,  2.52174437e-01,  4.00214672e-01,\n",
       "              -2.23040715e-01,  1.17412090e-01, -2.50236124e-01, -1.07839577e-01,\n",
       "              -4.65004519e-02, -6.59341037e-01, -1.04001045e+00,  5.11330843e-01,\n",
       "              -1.77059963e-01, -3.21875572e-01,  7.95485616e-01, -1.60628766e-01,\n",
       "              -2.74684012e-01, -5.65534174e-01, -1.04950440e+00, -3.79256427e-01],\n",
       "             dtype=float32)                                                       ,\n",
       "       array([ 0.5957881 ,  0.08991565, -0.2814699 ,  0.56761044,  0.17831746,\n",
       "               0.9313736 , -1.501741  , -0.7431988 ,  1.42691   ,  0.29006568,\n",
       "              -0.8605722 , -0.6864975 ,  0.21477623,  0.4610842 ,  0.58773917,\n",
       "              -0.64408547, -0.44760415, -0.54357827,  0.7583429 ,  1.1432294 ,\n",
       "               1.4002358 , -0.46118724, -0.2620444 ,  0.04217322, -0.7935785 ,\n",
       "               0.71947473, -0.04336397, -0.76866716, -0.17207065, -0.1721908 ,\n",
       "              -1.5795974 ,  0.10597852, -0.485982  ,  0.31806144,  0.98175997,\n",
       "              -0.22379264, -0.27849558,  0.66475314,  0.3698843 ,  0.8723738 ,\n",
       "              -0.5422113 , -0.18205422,  0.8468244 , -0.8754782 ,  0.10557055,\n",
       "              -0.47700828, -0.800105  , -0.5925215 , -0.6490921 , -0.704627  ,\n",
       "              -0.21254726,  1.1200486 , -0.76117367,  0.07214162, -0.14519817,\n",
       "              -1.25708   ,  1.3365217 , -0.7730344 ,  0.5717733 ,  0.14767271,\n",
       "               0.24681954,  0.6190534 ,  0.71451443,  0.28854784, -0.10718719,\n",
       "              -0.7768576 , -0.52447486, -0.695352  ,  1.3875461 ,  1.1647253 ,\n",
       "              -0.36730835, -0.10084025, -0.8083892 ,  0.9392811 , -0.6031119 ,\n",
       "              -0.28245834,  1.0362782 ,  0.28877318,  0.4323607 , -0.08117672,\n",
       "              -1.0750285 , -0.01952164, -0.79298043, -0.4469272 ,  0.24781097,\n",
       "               0.54770064, -0.17364325, -0.0148898 , -0.74327374,  0.5658404 ,\n",
       "              -0.5774572 , -0.77723587,  0.99540406, -0.37505934, -1.9282513 ,\n",
       "              -0.6582925 ,  0.8014575 , -0.09786899, -0.4540139 ,  0.35284463],\n",
       "             dtype=float32)                                                    ,\n",
       "       array([ 0.47258788,  0.34340197,  0.17385021, -0.03420144, -0.19108707,\n",
       "              -0.04033339, -0.26302186, -0.02029307,  0.01866277, -0.01079335,\n",
       "              -0.51167226, -0.09451275, -0.0883906 ,  0.24280721, -0.04088698,\n",
       "              -0.01238479, -0.5986642 , -0.01805467, -0.20430796,  0.09864791,\n",
       "               0.640851  ,  0.11974265, -0.8145806 , -0.13835852,  0.12754601,\n",
       "              -0.20134902,  0.21847238, -1.3637053 ,  0.03688686, -0.50231004,\n",
       "              -0.34714755, -0.1382624 ,  0.5614014 ,  0.25024098,  0.5960959 ,\n",
       "               0.11271194,  0.47140414,  0.42291656, -0.220637  , -0.39597502,\n",
       "               0.04280755,  0.33432266,  0.31423554,  0.49627003,  0.26722   ,\n",
       "               0.5413501 , -0.14360656, -0.33535352, -0.2849657 , -0.678403  ,\n",
       "              -0.18786637,  0.1596038 ,  0.31244966,  0.09694204, -0.25470275,\n",
       "              -0.25278422,  0.99208117, -0.42556155,  0.5670695 ,  0.5970853 ,\n",
       "               0.14934002,  0.03892344, -1.0916369 , -0.55694866,  0.05741474,\n",
       "               0.13965945, -0.23048158, -0.6873142 , -0.25935906, -0.04615925,\n",
       "              -0.28317192, -0.12112812, -0.04367537, -0.27256903,  0.25294596,\n",
       "              -0.27045894, -0.3802685 ,  0.49246666, -0.6126869 ,  0.3352452 ,\n",
       "              -0.67021745, -0.44773105, -0.57732856,  0.03509105,  0.26324332,\n",
       "               0.56860614,  0.01210117, -0.70139563,  0.2185341 , -0.13743389,\n",
       "              -0.89898884,  0.18715557,  0.3546533 , -0.06000282, -0.06723177,\n",
       "               0.10126812, -0.22614603, -0.62230295, -0.24167527,  0.23282638],\n",
       "             dtype=float32)                                                    ,\n",
       "       array([ 0.2398636 , -0.0290406 , -0.06604005, -0.21519269,  0.16934632,\n",
       "               0.77670467, -1.727021  ,  0.00728362,  0.25397575, -0.01589442,\n",
       "              -0.06521629, -0.38207817,  0.7010737 ,  0.2792952 , -0.10094909,\n",
       "               0.4334491 , -0.02097043, -0.34815654,  0.47211885,  0.5795019 ,\n",
       "               0.4303629 , -0.5310937 , -0.5961508 ,  0.19985911, -0.43151873,\n",
       "               0.42751715,  0.09049165, -0.8299069 ,  0.2581228 , -0.61066496,\n",
       "              -1.1055951 ,  0.48421812, -0.9558592 , -0.31930104,  0.6169124 ,\n",
       "              -0.1399142 , -0.30799407,  0.6992885 ,  0.1517384 ,  0.5537948 ,\n",
       "               0.01519841, -0.14149474,  0.04929101, -0.43405795,  0.5604165 ,\n",
       "              -0.13513274, -0.32027885, -0.86131513, -0.6882468 , -1.1282912 ,\n",
       "              -0.12012013,  0.57672906, -0.4201574 ,  0.19525242, -0.14091703,\n",
       "              -1.0764135 ,  0.82650936, -0.6719387 ,  0.33013186, -0.6073852 ,\n",
       "               0.418163  ,  0.03724576, -0.10465777, -0.41413397,  0.2943459 ,\n",
       "              -1.1004802 , -0.24153177, -0.61046875,  1.5426637 ,  0.7714767 ,\n",
       "               0.04053519, -0.24136311, -1.1376332 , -0.25321758, -0.4915496 ,\n",
       "               0.34264892, -0.0741277 ,  0.450801  ,  0.3600052 ,  0.25948444,\n",
       "              -0.7822962 ,  0.30585176, -0.7963946 , -0.789176  ,  0.42075235,\n",
       "               0.20314412, -0.36516875,  0.54973286, -0.43933156,  0.00857687,\n",
       "              -1.0955795 , -0.1637035 ,  1.1993423 , -0.5475263 , -1.6420193 ,\n",
       "              -0.5582763 ,  0.36547694,  0.14624158,  0.05804342, -0.21685204],\n",
       "             dtype=float32)                                                    ],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['embedding'].iloc[1:5].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(data['embedding'].tolist())\n",
    "y = data['class_index']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the classifier\n",
    "joblib.dump(clf, 'classifier_model.joblib')\n",
    "# To load the classifier later\n",
    "loaded_clf = joblib.load('classifier_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function for sentiment prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, word2vec_model, clf):\n",
    "    processed_text = preprocess_text(text)\n",
    "    embedding = get_document_embedding(processed_text, word2vec_model)\n",
    "    prediction = clf.predict([embedding])[0]\n",
    "    sentiment = \"Positive\" if prediction == 1 else \"Negative\"\n",
    "    return sentiment\n",
    "\n",
    "\n",
    "# Example usage\n",
    "sample_text = \"This product is amazing! I love it.\"\n",
    "result = predict_sentiment(sample_text, word2vec_model, clf)\n",
    "print(f\"Sentiment: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have already trained your Word2Vec model and classifier\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences=data['processed_text'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
