{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling\n",
    "\n",
    "### Step we follow:\n",
    "- Set up the environment and imported necessary libraries.\n",
    "- Loaded and preprocessed the 20 Newsgroups dataset.\n",
    "- Created a dictionary and corpus for Gensim.\n",
    "- Trained an LDA (Latent Dirichlet Allocation) model.\n",
    "- Evaluated the model using coherence score.\n",
    "- Visualized the topics using pyLDAvis.\n",
    "- Printed the top words for each topic.\n",
    "- Assigned topics to documents.\n",
    "- Analyzed and visualized the topic distribution.\n",
    "- Created a function to get the topic for new text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ram/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ram/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ram/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import dill\n",
    "import nltk\n",
    "import joblib\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess the 20 Newsgroups dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 20 Newsgroups dataset\n",
    "newsgroups = fetch_20newsgroups(\n",
    "    subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({'text': newsgroups.data, 'target': newsgroups.target})\n",
    "\n",
    "# Define preprocessing function\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Remove stopwords and non-alphabetic tokens\n",
    "    tokens = [token for token in tokens if token.isalpha()\n",
    "              and token not in STOPWORDS]\n",
    "\n",
    "    # Lemmatize tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Apply preprocessing to the text column\n",
    "df['processed_text'] = df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictionary and corpus for Gensim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary\n",
    "dictionary = corpora.Dictionary(df['processed_text'])\n",
    "\n",
    "# Filter out extreme words (appearing in less than 5 documents or more than 50% of documents)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "\n",
    "# Create the corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in df['processed_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the LDA model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set LDA parameters\n",
    "num_topics = 20\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None\n",
    "\n",
    "# Train the LDA model\n",
    "lda_model = LdaMulticore(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=num_topics,\n",
    "    random_state=42,\n",
    "    chunksize=chunksize,\n",
    "    passes=passes,\n",
    "    iterations=iterations,\n",
    "    eval_every=eval_every,\n",
    "    workers=4  # Adjust based on your CPU cores\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model using coherence score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.6204193118079318\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Calculate coherence score\n",
    "coherence_model_lda = CoherenceModel(\n",
    "    model=lda_model, texts=df['processed_text'], dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print(f'Coherence Score: {coherence_lda}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA visualization saved as 'lda_visualization.html'\n"
     ]
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "# Prepare the visualization\n",
    "vis_data = gensimvis.prepare(lda_model, corpus, dictionary)\n",
    "\n",
    "# Save the visualization as an HTML file\n",
    "pyLDAvis.save_html(vis_data, 'lda_visualization.html')\n",
    "print(\"LDA visualization saved as 'lda_visualization.html'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the top words for each topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0\n",
      "max, q, p, r, g, n, giz, bhj, m, w\n",
      "\n",
      "Topic: 1\n",
      "state, government, law, right, weapon, new, united, firearm, american, gun\n",
      "\n",
      "Topic: 2\n",
      "think, people, moral, right, value, human, society, yes, objective, point\n",
      "\n",
      "Topic: 3\n",
      "said, people, q, know, think, gun, time, child, president, going\n",
      "\n",
      "Topic: 4\n",
      "file, image, window, program, x, use, available, user, version, ftp\n",
      "\n",
      "Topic: 5\n",
      "m, g, r, p, c, b, n, w, s, o\n",
      "\n",
      "Topic: 6\n",
      "space, launch, nasa, mission, satellite, research, orbit, science, data, shuttle\n",
      "\n",
      "Topic: 7\n",
      "homosexual, sex, homosexuality, men, paul, church, male, sexual, gay, woman\n",
      "\n",
      "Topic: 8\n",
      "drive, card, disk, window, problem, use, know, thanks, work, like\n",
      "\n",
      "Topic: 9\n",
      "wire, power, use, ground, circuit, wiring, box, light, cable, outlet\n",
      "\n",
      "Topic: 10\n",
      "x, armenian, turkish, planet, turk, earth, year, entry, russian, genocide\n",
      "\n",
      "Topic: 11\n",
      "car, engine, new, know, like, problem, mile, tire, dealer, oil\n",
      "\n",
      "Topic: 12\n",
      "like, water, energy, time, battery, think, greek, american, right, mean\n",
      "\n",
      "Topic: 13\n",
      "book, theory, atheist, universe, do, support, version, mouse, argument, network\n",
      "\n",
      "Topic: 14\n",
      "game, team, year, player, play, run, time, season, fan, hit\n",
      "\n",
      "Topic: 15\n",
      "key, use, drug, chip, encryption, number, information, government, privacy, clipper\n",
      "\n",
      "Topic: 16\n",
      "like, people, think, know, good, time, thing, year, want, going\n",
      "\n",
      "Topic: 17\n",
      "team, hockey, nhl, new, league, la, game, pt, season, player\n",
      "\n",
      "Topic: 18\n",
      "jew, israel, israeli, arab, jewish, people, war, country, palestinian, right\n",
      "\n",
      "Topic: 19\n",
      "god, people, christian, jesus, believe, word, know, bible, church, belief\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_topics(lda_model, num_words=10):\n",
    "    for idx, topic in lda_model.print_topics(-1, num_words):\n",
    "        print(f\"Topic: {idx}\")\n",
    "        print(\", \".join([word.split(\"*\")[1].strip().replace('\"', '')\n",
    "              for word in topic.split(\"+\")]))\n",
    "        print()\n",
    "\n",
    "\n",
    "print_topics(lda_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign topics to documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dominant_topic(lda_model, corpus):\n",
    "    topic_assignments = []\n",
    "    for doc in corpus:\n",
    "        topic_dist = lda_model.get_document_topics(doc)\n",
    "        dominant_topic = max(topic_dist, key=lambda x: x[1])[0]\n",
    "        topic_assignments.append(dominant_topic)\n",
    "    return topic_assignments\n",
    "\n",
    "\n",
    "df['dominant_topic'] = get_dominant_topic(lda_model, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze topic distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic distribution plot saved as 'topic_distribution.png'\n"
     ]
    }
   ],
   "source": [
    "topic_distribution = df['dominant_topic'].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "topic_distribution.plot(kind='bar')\n",
    "plt.title('Topic Distribution')\n",
    "plt.xlabel('Topic')\n",
    "plt.ylabel('Number of Documents')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('topic_distribution.png')\n",
    "plt.close()\n",
    "print(\"Topic distribution plot saved as 'topic_distribution.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function to get topic for new text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant topic for the new text: 4\n"
     ]
    }
   ],
   "source": [
    "def get_topic_for_text(text, lda_model, dictionary):\n",
    "    processed_text = preprocess_text(text)\n",
    "    bow = dictionary.doc2bow(processed_text)\n",
    "    topic_dist = lda_model.get_document_topics(bow)\n",
    "    dominant_topic = max(topic_dist, key=lambda x: x[1])[0]\n",
    "    return dominant_topic\n",
    "\n",
    "\n",
    "# Example usage\n",
    "new_text = \"This is a sample text about computer science and programming.\"\n",
    "topic = get_topic_for_text(new_text, lda_model, dictionary)\n",
    "print(f\"Dominant topic for the new text: {topic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model and other neccessary stuff for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All topic model components saved successfully.\n"
     ]
    }
   ],
   "source": [
    "def save_topic_model_components(lda_model, dictionary, corpus, df, preprocess_text_func, base_path=\"topic_model_data\"):\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "    joblib.dump(lda_model, os.path.join(base_path, \"lda_model.joblib\"))\n",
    "    joblib.dump(dictionary, os.path.join(base_path, \"dictionary.joblib\"))\n",
    "    joblib.dump(corpus, os.path.join(base_path, \"corpus.joblib\"))\n",
    "    joblib.dump(df, os.path.join(base_path, \"preprocessed_df.joblib\"))\n",
    "\n",
    "    with open(os.path.join(base_path, \"preprocess_text_function.dill\"), \"wb\") as f:\n",
    "        dill.dump(preprocess_text_func, f)\n",
    "\n",
    "    print(\"All topic model components saved successfully.\")\n",
    "\n",
    "\n",
    "# Usage\n",
    "save_topic_model_components(lda_model, dictionary, corpus, df, preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To load the saved components later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All topic model components loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "def load_topic_model_components(base_path=\"topic_model_data\"):\n",
    "    lda_model = joblib.load(os.path.join(base_path, \"lda_model.joblib\"))\n",
    "    dictionary = joblib.load(os.path.join(base_path, \"dictionary.joblib\"))\n",
    "    corpus = joblib.load(os.path.join(base_path, \"corpus.joblib\"))\n",
    "    df = joblib.load(os.path.join(base_path, \"preprocessed_df.joblib\"))\n",
    "\n",
    "    with open(os.path.join(base_path, \"preprocess_text_function.dill\"), \"rb\") as f:\n",
    "        preprocess_text_func = dill.load(f)\n",
    "\n",
    "    print(\"All topic model components loaded successfully.\")\n",
    "    return lda_model, dictionary, corpus, df, preprocess_text_func\n",
    "\n",
    "\n",
    "# Usage\n",
    "loaded_lda_model, loaded_dictionary, loaded_corpus, loaded_df, loaded_preprocess_text = load_topic_model_components()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0\n",
      "max, q, p, r, g, n, giz, bhj, m, w\n",
      "\n",
      "Topic: 1\n",
      "state, government, law, right, weapon, new, united, firearm, american, gun\n",
      "\n",
      "Topic: 2\n",
      "think, people, moral, right, value, human, society, yes, objective, point\n",
      "\n",
      "Topic: 3\n",
      "said, people, q, know, think, gun, time, child, president, going\n",
      "\n",
      "Topic: 4\n",
      "file, image, window, program, x, use, available, user, version, ftp\n",
      "\n",
      "Topic: 5\n",
      "m, g, r, p, c, b, n, w, s, o\n",
      "\n",
      "Topic: 6\n",
      "space, launch, nasa, mission, satellite, research, orbit, science, data, shuttle\n",
      "\n",
      "Topic: 7\n",
      "homosexual, sex, homosexuality, men, paul, church, male, sexual, gay, woman\n",
      "\n",
      "Topic: 8\n",
      "drive, card, disk, window, problem, use, know, thanks, work, like\n",
      "\n",
      "Topic: 9\n",
      "wire, power, use, ground, circuit, wiring, box, light, cable, outlet\n",
      "\n",
      "Topic: 10\n",
      "x, armenian, turkish, planet, turk, earth, year, entry, russian, genocide\n",
      "\n",
      "Topic: 11\n",
      "car, engine, new, know, like, problem, mile, tire, dealer, oil\n",
      "\n",
      "Topic: 12\n",
      "like, water, energy, time, battery, think, greek, american, right, mean\n",
      "\n",
      "Topic: 13\n",
      "book, theory, atheist, universe, do, support, version, mouse, argument, network\n",
      "\n",
      "Topic: 14\n",
      "game, team, year, player, play, run, time, season, fan, hit\n",
      "\n",
      "Topic: 15\n",
      "key, use, drug, chip, encryption, number, information, government, privacy, clipper\n",
      "\n",
      "Topic: 16\n",
      "like, people, think, know, good, time, thing, year, want, going\n",
      "\n",
      "Topic: 17\n",
      "team, hockey, nhl, new, league, la, game, pt, season, player\n",
      "\n",
      "Topic: 18\n",
      "jew, israel, israeli, arab, jewish, people, war, country, palestinian, right\n",
      "\n",
      "Topic: 19\n",
      "god, people, christian, jesus, believe, word, know, bible, church, belief\n",
      "\n",
      "Dominant topic for the new text: 4\n"
     ]
    }
   ],
   "source": [
    "# Example: Print topics using the loaded model\n",
    "def print_topics(lda_model, num_words=10):\n",
    "    for idx, topic in lda_model.print_topics(-1, num_words):\n",
    "        print(f\"Topic: {idx}\")\n",
    "        print(\", \".join([word.split(\"*\")[1].strip().replace('\"', '')\n",
    "              for word in topic.split(\"+\")]))\n",
    "        print()\n",
    "\n",
    "\n",
    "print_topics(loaded_lda_model)\n",
    "\n",
    "# Example: Get topic for new text using loaded components\n",
    "\n",
    "\n",
    "def get_topic_for_text(text, lda_model, dictionary, preprocess_func):\n",
    "    processed_text = preprocess_func(text)\n",
    "    bow = dictionary.doc2bow(processed_text)\n",
    "    topic_dist = lda_model.get_document_topics(bow)\n",
    "    dominant_topic = max(topic_dist, key=lambda x: x[1])[0]\n",
    "    return dominant_topic\n",
    "\n",
    "\n",
    "new_text = \"This is a sample text about computer science and programming.\"\n",
    "topic = get_topic_for_text(new_text, loaded_lda_model,\n",
    "                           loaded_dictionary, loaded_preprocess_text)\n",
    "print(f\"Dominant topic for the new text: {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-v] [-q] [--locals] [-f] [-c] [-b]\n",
      "                             [-k TESTNAMEPATTERNS]\n",
      "                             [tests ...]\n",
      "ipykernel_launcher.py: error: argument -f/--failfast: ignored explicit argument '/home/ram/.local/share/jupyter/runtime/kernel-v33878badd108547c084ff41c9f1bf87ea8239424a.json'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'tb_frame'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/python3.10/argparse.py:1878\u001b[0m, in \u001b[0;36mArgumentParser.parse_known_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1877\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1878\u001b[0m     namespace, args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_known_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1879\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ArgumentError:\n",
      "File \u001b[0;32m/usr/lib/python3.10/argparse.py:2091\u001b[0m, in \u001b[0;36mArgumentParser._parse_known_args\u001b[0;34m(self, arg_strings, namespace)\u001b[0m\n\u001b[1;32m   2090\u001b[0m     \u001b[38;5;66;03m# consume the next optional and any arguments for it\u001b[39;00m\n\u001b[0;32m-> 2091\u001b[0m     start_index \u001b[38;5;241m=\u001b[39m \u001b[43mconsume_optional\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2093\u001b[0m \u001b[38;5;66;03m# consume any positionals following the last Optional\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/argparse.py:2013\u001b[0m, in \u001b[0;36mArgumentParser._parse_known_args.<locals>.consume_optional\u001b[0;34m(start_index)\u001b[0m\n\u001b[1;32m   2012\u001b[0m         msg \u001b[38;5;241m=\u001b[39m _(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignored explicit argument \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 2013\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ArgumentError(action, msg \u001b[38;5;241m%\u001b[39m explicit_arg)\n\u001b[1;32m   2015\u001b[0m \u001b[38;5;66;03m# if there is no explicit argument, try to match the\u001b[39;00m\n\u001b[1;32m   2016\u001b[0m \u001b[38;5;66;03m# optional's string arguments with the following strings\u001b[39;00m\n\u001b[1;32m   2017\u001b[0m \u001b[38;5;66;03m# if successful, exit the loop\u001b[39;00m\n\u001b[1;32m   2018\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mArgumentError\u001b[0m: argument -f/--failfast: ignored explicit argument '/home/ram/.local/share/jupyter/runtime/kernel-v33878badd108547c084ff41c9f1bf87ea8239424a.json'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[23], line 90\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massertTrue(\u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(word_id, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(count, \u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m     87\u001b[0m                             \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus \u001b[38;5;28;01mfor\u001b[39;00m word_id, count \u001b[38;5;129;01min\u001b[39;00m doc))\n\u001b[0;32m---> 90\u001b[0m \u001b[43munittest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/unittest/main.py:100\u001b[0m, in \u001b[0;36mTestProgram.__init__\u001b[0;34m(self, module, defaultTest, argv, testRunner, testLoader, exit, verbosity, failfast, catchbreak, buffer, warnings, tb_locals)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogName \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(argv[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 100\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparseArgs\u001b[49m\u001b[43m(\u001b[49m\u001b[43margv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunTests()\n",
      "File \u001b[0;32m/usr/lib/python3.10/unittest/main.py:133\u001b[0m, in \u001b[0;36mTestProgram.parseArgs\u001b[0;34m(self, argv)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_main_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtests:\n",
      "File \u001b[0;32m/usr/lib/python3.10/argparse.py:1845\u001b[0m, in \u001b[0;36mArgumentParser.parse_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1844\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_args\u001b[39m(\u001b[38;5;28mself\u001b[39m, args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, namespace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1845\u001b[0m     args, argv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_known_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1846\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m argv:\n",
      "File \u001b[0;32m/usr/lib/python3.10/argparse.py:1881\u001b[0m, in \u001b[0;36mArgumentParser.parse_known_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1880\u001b[0m         err \u001b[38;5;241m=\u001b[39m _sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m-> 1881\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/argparse.py:2606\u001b[0m, in \u001b[0;36mArgumentParser.error\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m   2605\u001b[0m args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprog\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprog, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m: message}\n\u001b[0;32m-> 2606\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m%(prog)s\u001b[39;49;00m\u001b[38;5;124;43m: error: \u001b[39;49m\u001b[38;5;132;43;01m%(message)s\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/argparse.py:2593\u001b[0m, in \u001b[0;36mArgumentParser.exit\u001b[0;34m(self, status, message)\u001b[0m\n\u001b[1;32m   2592\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print_message(message, _sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m-> 2593\u001b[0m \u001b[43m_sys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mSystemExit\u001b[0m: 2",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/convo-insight-platform/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2145\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_only:\n\u001b[1;32m   2143\u001b[0m     stb \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAn exception has occurred, use \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mtb to see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2144\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe full traceback.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m-> 2145\u001b[0m     stb\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInteractiveTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_exception_only\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2146\u001b[0m \u001b[43m                                                     \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2149\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontains_exceptiongroup\u001b[39m(val):\n",
      "File \u001b[0;32m~/convo-insight-platform/venv/lib/python3.10/site-packages/IPython/core/ultratb.py:710\u001b[0m, in \u001b[0;36mListTB.get_exception_only\u001b[0;34m(self, etype, value)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_exception_only\u001b[39m(\u001b[38;5;28mself\u001b[39m, etype, value):\n\u001b[1;32m    703\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Only print the exception type and message, without a traceback.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \n\u001b[1;32m    705\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;124;03m    value : exception value\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mListTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/convo-insight-platform/venv/lib/python3.10/site-packages/IPython/core/ultratb.py:568\u001b[0m, in \u001b[0;36mListTB.structured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[1;32m    565\u001b[0m     chained_exc_ids\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mid\u001b[39m(exception[\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m    566\u001b[0m     chained_exceptions_tb_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    567\u001b[0m     out_list \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 568\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m            \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m            \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchained_exc_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    572\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchained_exceptions_tb_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;241m+\u001b[39m chained_exception_message\n\u001b[1;32m    576\u001b[0m         \u001b[38;5;241m+\u001b[39m out_list)\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_list\n",
      "File \u001b[0;32m~/convo-insight-platform/venv/lib/python3.10/site-packages/IPython/core/ultratb.py:1454\u001b[0m, in \u001b[0;36mAutoFormattedTB.structured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1452\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1453\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtb \u001b[38;5;241m=\u001b[39m etb\n\u001b[0;32m-> 1454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFormattedTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1455\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\n\u001b[1;32m   1456\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/convo-insight-platform/venv/lib/python3.10/site-packages/IPython/core/ultratb.py:1345\u001b[0m, in \u001b[0;36mFormattedTB.structured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1342\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose_modes:\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;66;03m# Verbose modes need a full traceback\u001b[39;00m\n\u001b[0;32m-> 1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVerboseTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\n\u001b[1;32m   1347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMinimal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ListTB\u001b[38;5;241m.\u001b[39mget_exception_only(\u001b[38;5;28mself\u001b[39m, etype, value)\n",
      "File \u001b[0;32m~/convo-insight-platform/venv/lib/python3.10/site-packages/IPython/core/ultratb.py:1192\u001b[0m, in \u001b[0;36mVerboseTB.structured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstructured_traceback\u001b[39m(\n\u001b[1;32m   1184\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1185\u001b[0m     etype: \u001b[38;5;28mtype\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     number_of_lines_of_context: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m   1190\u001b[0m ):\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1192\u001b[0m     formatted_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_exception_as_a_whole\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m     colors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mColors  \u001b[38;5;66;03m# just a shorthand + quicker name lookup\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m     colorsnormal \u001b[38;5;241m=\u001b[39m colors\u001b[38;5;241m.\u001b[39mNormal  \u001b[38;5;66;03m# used a lot\u001b[39;00m\n",
      "File \u001b[0;32m~/convo-insight-platform/venv/lib/python3.10/site-packages/IPython/core/ultratb.py:1082\u001b[0m, in \u001b[0;36mVerboseTB.format_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tb_offset, \u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m   1080\u001b[0m head \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_header(\u001b[38;5;28mstr\u001b[39m(etype), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlong_header)\n\u001b[1;32m   1081\u001b[0m records \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1082\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m etb \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m   1083\u001b[0m )\n\u001b[1;32m   1085\u001b[0m frames \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1086\u001b[0m skipped \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/convo-insight-platform/venv/lib/python3.10/site-packages/IPython/core/ultratb.py:1150\u001b[0m, in \u001b[0;36mVerboseTB.get_records\u001b[0;34m(self, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1150\u001b[0m         mod \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetmodule(\u001b[43mcf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtb_frame\u001b[49m)\n\u001b[1;32m   1151\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mod \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1152\u001b[0m             mod_name \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'tb_frame'"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import joblib\n",
    "import dill\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim import corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "\n",
    "class TestLDATopicModeling(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        # Load the trained model and necessary data\n",
    "        cls.lda_model = joblib.load(\"topic_model_data/lda_model.joblib\")\n",
    "        cls.dictionary = joblib.load(\"topic_model_data/dictionary.joblib\")\n",
    "        cls.corpus = joblib.load(\"topic_model_data/corpus.joblib\")\n",
    "        cls.df = joblib.load(\"topic_model_data/preprocessed_df.joblib\")\n",
    "\n",
    "        with open(\"topic_model_data/preprocess_text_function.dill\", \"rb\") as f:\n",
    "            cls.preprocess_text = dill.load(f)\n",
    "\n",
    "        # Load a small test set\n",
    "        cls.test_texts = cls.df['text'].head(100).tolist()\n",
    "        cls.test_processed = [cls.preprocess_text(\n",
    "            text) for text in cls.test_texts]\n",
    "\n",
    "    def test_model_type(self):\n",
    "        self.assertIsInstance(self.lda_model, LdaMulticore)\n",
    "        self.assertIsInstance(self.dictionary, corpora.Dictionary)\n",
    "        self.assertIsInstance(self.corpus, list)\n",
    "\n",
    "    def test_model_output_format(self):\n",
    "        for doc in self.test_processed[:5]:  # Test with first 5 documents\n",
    "            bow = self.dictionary.doc2bow(doc)\n",
    "            topics = self.lda_model.get_document_topics(bow)\n",
    "            self.assertTrue(all(isinstance(topic[0], int) and isinstance(\n",
    "                topic[1], float) for topic in topics))\n",
    "            self.assertTrue(all(0 <= prob <= 1 for _, prob in topics))\n",
    "\n",
    "    def test_topic_coherence(self):\n",
    "        coherence_model = CoherenceModel(\n",
    "            model=self.lda_model, texts=self.test_processed, dictionary=self.dictionary, coherence='c_v')\n",
    "        coherence_score = coherence_model.get_coherence()\n",
    "        self.assertGreater(coherence_score, 0.0)  # Adjust threshold as needed\n",
    "\n",
    "    def test_topic_diversity(self):\n",
    "        N = 20\n",
    "        topic_words = [word for topic in self.lda_model.show_topics(num_topics=-1, num_words=N, formatted=False)\n",
    "                       for word, _ in topic[1]]\n",
    "        diversity_score = len(set(topic_words)) / \\\n",
    "            (self.lda_model.num_topics * N)\n",
    "        self.assertGreater(diversity_score, 0.0)  # Adjust threshold as needed\n",
    "\n",
    "    def test_new_text_assignment(self):\n",
    "        new_text = \"This is a sample text about artificial intelligence and machine learning.\"\n",
    "        processed_text = self.preprocess_text(new_text)\n",
    "        bow = self.dictionary.doc2bow(processed_text)\n",
    "        topics = self.lda_model.get_document_topics(bow)\n",
    "        self.assertTrue(len(topics) > 0)\n",
    "        self.assertIsInstance(topics[0][0], int)\n",
    "        self.assertIsInstance(topics[0][1], float)\n",
    "        self.assertTrue(0 <= topics[0][1] <= 1)\n",
    "\n",
    "    def test_model_consistency(self):\n",
    "        text = \"This is a test text for consistency.\"\n",
    "        processed_text = self.preprocess_text(text)\n",
    "        bow = self.dictionary.doc2bow(processed_text)\n",
    "        topics1 = self.lda_model.get_document_topics(bow)\n",
    "        topics2 = self.lda_model.get_document_topics(bow)\n",
    "        self.assertEqual(topics1, topics2)\n",
    "\n",
    "    def test_preprocessing_function(self):\n",
    "        text = \"This is a TEST sentence with UPPERCASE words and punctuation!\"\n",
    "        processed = self.preprocess_text(text)\n",
    "        self.assertTrue(all(word.islower() for word in processed))\n",
    "        self.assertTrue(all(word.isalpha() for word in processed))\n",
    "\n",
    "    def test_dictionary_filter(self):\n",
    "        # Check if extreme words have been filtered out\n",
    "        self.assertTrue(\n",
    "            all(self.dictionary.dfs[id] >= 5 for id in self.dictionary.dfs))\n",
    "        self.assertTrue(all(\n",
    "            self.dictionary.dfs[id] / len(self.corpus) <= 0.5 for id in self.dictionary.dfs))\n",
    "\n",
    "    def test_corpus_format(self):\n",
    "        self.assertTrue(all(isinstance(doc, list)\n",
    "                        for doc in self.corpus[:5]))  # Test with first 5 documents\n",
    "        self.assertTrue(all(isinstance(word_id, int) and isinstance(count, int)\n",
    "                            for doc in self.corpus[:5] for word_id, count in doc))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>dominant_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nI am sure some bashers of Pens fans are pr...</td>\n",
       "      <td>10</td>\n",
       "      <td>[sure, bashers, pen, fan, pretty, confused, la...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My brother is in the market for a high-perform...</td>\n",
       "      <td>3</td>\n",
       "      <td>[brother, market, video, card, support, vesa, ...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n\\n\\n\\tFinally you said what you dream abou...</td>\n",
       "      <td>17</td>\n",
       "      <td>[finally, said, dream, mediterranean, new, are...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nThink!\\n\\nIt's the SCSI card doing the DMA t...</td>\n",
       "      <td>3</td>\n",
       "      <td>[think, scsi, card, dma, transfer, disk, scsi,...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1)    I have an old Jasmine drive which I cann...</td>\n",
       "      <td>4</td>\n",
       "      <td>[old, jasmine, drive, use, new, understanding,...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0  \\n\\nI am sure some bashers of Pens fans are pr...      10   \n",
       "1  My brother is in the market for a high-perform...       3   \n",
       "2  \\n\\n\\n\\n\\tFinally you said what you dream abou...      17   \n",
       "3  \\nThink!\\n\\nIt's the SCSI card doing the DMA t...       3   \n",
       "4  1)    I have an old Jasmine drive which I cann...       4   \n",
       "\n",
       "                                      processed_text  dominant_topic  \n",
       "0  [sure, bashers, pen, fan, pretty, confused, la...              14  \n",
       "1  [brother, market, video, card, support, vesa, ...               8  \n",
       "2  [finally, said, dream, mediterranean, new, are...               3  \n",
       "3  [think, scsi, card, dma, transfer, disk, scsi,...               8  \n",
       "4  [old, jasmine, drive, use, new, understanding,...               8  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized_text'] = df['text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>dominant_topic</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nI am sure some bashers of Pens fans are pr...</td>\n",
       "      <td>10</td>\n",
       "      <td>[sure, bashers, pen, fan, pretty, confused, la...</td>\n",
       "      <td>14</td>\n",
       "      <td>[I, am, sure, some, bashers, of, Pens, fans, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My brother is in the market for a high-perform...</td>\n",
       "      <td>3</td>\n",
       "      <td>[brother, market, video, card, support, vesa, ...</td>\n",
       "      <td>8</td>\n",
       "      <td>[My, brother, is, in, the, market, for, a, hig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n\\n\\n\\tFinally you said what you dream abou...</td>\n",
       "      <td>17</td>\n",
       "      <td>[finally, said, dream, mediterranean, new, are...</td>\n",
       "      <td>3</td>\n",
       "      <td>[Finally, you, said, what, you, dream, about, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nThink!\\n\\nIt's the SCSI card doing the DMA t...</td>\n",
       "      <td>3</td>\n",
       "      <td>[think, scsi, card, dma, transfer, disk, scsi,...</td>\n",
       "      <td>8</td>\n",
       "      <td>[Think, !, It, 's, the, SCSI, card, doing, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1)    I have an old Jasmine drive which I cann...</td>\n",
       "      <td>4</td>\n",
       "      <td>[old, jasmine, drive, use, new, understanding,...</td>\n",
       "      <td>8</td>\n",
       "      <td>[1, ), I, have, an, old, Jasmine, drive, which...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0  \\n\\nI am sure some bashers of Pens fans are pr...      10   \n",
       "1  My brother is in the market for a high-perform...       3   \n",
       "2  \\n\\n\\n\\n\\tFinally you said what you dream abou...      17   \n",
       "3  \\nThink!\\n\\nIt's the SCSI card doing the DMA t...       3   \n",
       "4  1)    I have an old Jasmine drive which I cann...       4   \n",
       "\n",
       "                                      processed_text  dominant_topic  \\\n",
       "0  [sure, bashers, pen, fan, pretty, confused, la...              14   \n",
       "1  [brother, market, video, card, support, vesa, ...               8   \n",
       "2  [finally, said, dream, mediterranean, new, are...               3   \n",
       "3  [think, scsi, card, dma, transfer, disk, scsi,...               8   \n",
       "4  [old, jasmine, drive, use, new, understanding,...               8   \n",
       "\n",
       "                                      tokenized_text  \n",
       "0  [I, am, sure, some, bashers, of, Pens, fans, a...  \n",
       "1  [My, brother, is, in, the, market, for, a, hig...  \n",
       "2  [Finally, you, said, what, you, dream, about, ...  \n",
       "3  [Think, !, It, 's, the, SCSI, card, doing, the...  \n",
       "4  [1, ), I, have, an, old, Jasmine, drive, which...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-v] [-q] [--locals] [-f] [-c] [-b]\n",
      "                             [-k TESTNAMEPATTERNS]\n",
      "                             [tests ...]\n",
      "ipykernel_launcher.py: error: argument -f/--failfast: ignored explicit argument '/home/ram/.local/share/jupyter/runtime/kernel-v33878badd108547c084ff41c9f1bf87ea8239424a.json'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'tb_frame'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/python3.10/argparse.py:1878\u001b[0m, in \u001b[0;36mArgumentParser.parse_known_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1877\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1878\u001b[0m     namespace, args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_known_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1879\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ArgumentError:\n",
      "File \u001b[0;32m/usr/lib/python3.10/argparse.py:2091\u001b[0m, in \u001b[0;36mArgumentParser._parse_known_args\u001b[0;34m(self, arg_strings, namespace)\u001b[0m\n\u001b[1;32m   2090\u001b[0m     \u001b[38;5;66;03m# consume the next optional and any arguments for it\u001b[39;00m\n\u001b[0;32m-> 2091\u001b[0m     start_index \u001b[38;5;241m=\u001b[39m \u001b[43mconsume_optional\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2093\u001b[0m \u001b[38;5;66;03m# consume any positionals following the last Optional\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/argparse.py:2013\u001b[0m, in \u001b[0;36mArgumentParser._parse_known_args.<locals>.consume_optional\u001b[0;34m(start_index)\u001b[0m\n\u001b[1;32m   2012\u001b[0m         msg \u001b[38;5;241m=\u001b[39m _(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignored explicit argument \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 2013\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ArgumentError(action, msg \u001b[38;5;241m%\u001b[39m explicit_arg)\n\u001b[1;32m   2015\u001b[0m \u001b[38;5;66;03m# if there is no explicit argument, try to match the\u001b[39;00m\n\u001b[1;32m   2016\u001b[0m \u001b[38;5;66;03m# optional's string arguments with the following strings\u001b[39;00m\n\u001b[1;32m   2017\u001b[0m \u001b[38;5;66;03m# if successful, exit the loop\u001b[39;00m\n\u001b[1;32m   2018\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mArgumentError\u001b[0m: argument -f/--failfast: ignored explicit argument '/home/ram/.local/share/jupyter/runtime/kernel-v33878badd108547c084ff41c9f1bf87ea8239424a.json'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[28], line 103\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massertTrue(\u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(word_id, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(count, \u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m    100\u001b[0m                             \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus[:\u001b[38;5;241m5\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m word_id, count \u001b[38;5;129;01min\u001b[39;00m doc))\n\u001b[0;32m--> 103\u001b[0m \u001b[43munittest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/unittest/main.py:100\u001b[0m, in \u001b[0;36mTestProgram.__init__\u001b[0;34m(self, module, defaultTest, argv, testRunner, testLoader, exit, verbosity, failfast, catchbreak, buffer, warnings, tb_locals)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogName \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(argv[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 100\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparseArgs\u001b[49m\u001b[43m(\u001b[49m\u001b[43margv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunTests()\n",
      "File \u001b[0;32m/usr/lib/python3.10/unittest/main.py:133\u001b[0m, in \u001b[0;36mTestProgram.parseArgs\u001b[0;34m(self, argv)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_main_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtests:\n",
      "File \u001b[0;32m/usr/lib/python3.10/argparse.py:1845\u001b[0m, in \u001b[0;36mArgumentParser.parse_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1844\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_args\u001b[39m(\u001b[38;5;28mself\u001b[39m, args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, namespace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1845\u001b[0m     args, argv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_known_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1846\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m argv:\n",
      "File \u001b[0;32m/usr/lib/python3.10/argparse.py:1881\u001b[0m, in \u001b[0;36mArgumentParser.parse_known_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1880\u001b[0m         err \u001b[38;5;241m=\u001b[39m _sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m-> 1881\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/argparse.py:2606\u001b[0m, in \u001b[0;36mArgumentParser.error\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m   2605\u001b[0m args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprog\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprog, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m: message}\n\u001b[0;32m-> 2606\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m%(prog)s\u001b[39;49;00m\u001b[38;5;124;43m: error: \u001b[39;49m\u001b[38;5;132;43;01m%(message)s\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/argparse.py:2593\u001b[0m, in \u001b[0;36mArgumentParser.exit\u001b[0;34m(self, status, message)\u001b[0m\n\u001b[1;32m   2592\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print_message(message, _sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m-> 2593\u001b[0m \u001b[43m_sys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mSystemExit\u001b[0m: 2",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/convo-insight-platform/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2145\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_only:\n\u001b[1;32m   2143\u001b[0m     stb \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAn exception has occurred, use \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mtb to see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2144\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe full traceback.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m-> 2145\u001b[0m     stb\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInteractiveTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_exception_only\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2146\u001b[0m \u001b[43m                                                     \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2149\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontains_exceptiongroup\u001b[39m(val):\n",
      "File \u001b[0;32m~/convo-insight-platform/venv/lib/python3.10/site-packages/IPython/core/ultratb.py:710\u001b[0m, in \u001b[0;36mListTB.get_exception_only\u001b[0;34m(self, etype, value)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_exception_only\u001b[39m(\u001b[38;5;28mself\u001b[39m, etype, value):\n\u001b[1;32m    703\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Only print the exception type and message, without a traceback.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \n\u001b[1;32m    705\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;124;03m    value : exception value\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mListTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/convo-insight-platform/venv/lib/python3.10/site-packages/IPython/core/ultratb.py:568\u001b[0m, in \u001b[0;36mListTB.structured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[1;32m    565\u001b[0m     chained_exc_ids\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mid\u001b[39m(exception[\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m    566\u001b[0m     chained_exceptions_tb_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    567\u001b[0m     out_list \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 568\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m            \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m            \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchained_exc_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    572\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchained_exceptions_tb_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;241m+\u001b[39m chained_exception_message\n\u001b[1;32m    576\u001b[0m         \u001b[38;5;241m+\u001b[39m out_list)\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_list\n",
      "File \u001b[0;32m~/convo-insight-platform/venv/lib/python3.10/site-packages/IPython/core/ultratb.py:1454\u001b[0m, in \u001b[0;36mAutoFormattedTB.structured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1452\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1453\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtb \u001b[38;5;241m=\u001b[39m etb\n\u001b[0;32m-> 1454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFormattedTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1455\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\n\u001b[1;32m   1456\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/convo-insight-platform/venv/lib/python3.10/site-packages/IPython/core/ultratb.py:1345\u001b[0m, in \u001b[0;36mFormattedTB.structured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1342\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose_modes:\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;66;03m# Verbose modes need a full traceback\u001b[39;00m\n\u001b[0;32m-> 1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVerboseTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\n\u001b[1;32m   1347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMinimal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ListTB\u001b[38;5;241m.\u001b[39mget_exception_only(\u001b[38;5;28mself\u001b[39m, etype, value)\n",
      "File \u001b[0;32m~/convo-insight-platform/venv/lib/python3.10/site-packages/IPython/core/ultratb.py:1192\u001b[0m, in \u001b[0;36mVerboseTB.structured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstructured_traceback\u001b[39m(\n\u001b[1;32m   1184\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1185\u001b[0m     etype: \u001b[38;5;28mtype\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     number_of_lines_of_context: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m   1190\u001b[0m ):\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1192\u001b[0m     formatted_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_exception_as_a_whole\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m     colors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mColors  \u001b[38;5;66;03m# just a shorthand + quicker name lookup\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m     colorsnormal \u001b[38;5;241m=\u001b[39m colors\u001b[38;5;241m.\u001b[39mNormal  \u001b[38;5;66;03m# used a lot\u001b[39;00m\n",
      "File \u001b[0;32m~/convo-insight-platform/venv/lib/python3.10/site-packages/IPython/core/ultratb.py:1082\u001b[0m, in \u001b[0;36mVerboseTB.format_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tb_offset, \u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m   1080\u001b[0m head \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_header(\u001b[38;5;28mstr\u001b[39m(etype), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlong_header)\n\u001b[1;32m   1081\u001b[0m records \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1082\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m etb \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m   1083\u001b[0m )\n\u001b[1;32m   1085\u001b[0m frames \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1086\u001b[0m skipped \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/convo-insight-platform/venv/lib/python3.10/site-packages/IPython/core/ultratb.py:1150\u001b[0m, in \u001b[0;36mVerboseTB.get_records\u001b[0;34m(self, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1150\u001b[0m         mod \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetmodule(\u001b[43mcf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtb_frame\u001b[49m)\n\u001b[1;32m   1151\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mod \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1152\u001b[0m             mod_name \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'tb_frame'"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import joblib\n",
    "import dill\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim import corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "\n",
    "class TestLDATopicModeling(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        # Load the trained model and necessary data\n",
    "        cls.lda_model = joblib.load(\"topic_model_data/lda_model.joblib\")\n",
    "        cls.dictionary = joblib.load(\"topic_model_data/dictionary.joblib\")\n",
    "        cls.corpus = joblib.load(\"topic_model_data/corpus.joblib\")\n",
    "        cls.df = joblib.load(\"topic_model_data/preprocessed_df.joblib\")\n",
    "\n",
    "        with open(\"topic_model_data/preprocess_text_function.dill\", \"rb\") as f:\n",
    "            cls.preprocess_text = dill.load(f)\n",
    "\n",
    "        # Load a small test set, now using the pre-tokenized texts\n",
    "        cls.test_texts = cls.df['tokenized_text'].head(100).tolist()\n",
    "        cls.test_processed = [cls.preprocess_text(\n",
    "            text) for text in cls.test_texts]\n",
    "\n",
    "    def test_model_type(self):\n",
    "        self.assertIsInstance(self.lda_model, LdaMulticore)\n",
    "        self.assertIsInstance(self.dictionary, corpora.Dictionary)\n",
    "        self.assertIsInstance(self.corpus, list)\n",
    "\n",
    "    def test_model_output_format(self):\n",
    "        for doc in self.test_processed[:5]:  # Test with first 5 documents\n",
    "            bow = self.dictionary.doc2bow(doc)\n",
    "            topics = self.lda_model.get_document_topics(bow)\n",
    "            self.assertTrue(all(isinstance(topic[0], int) and isinstance(\n",
    "                topic[1], float) for topic in topics))\n",
    "            self.assertTrue(all(0 <= prob <= 1 for _, prob in topics))\n",
    "\n",
    "    def test_topic_coherence(self):\n",
    "        coherence_model = CoherenceModel(\n",
    "            model=self.lda_model, texts=self.test_processed, dictionary=self.dictionary, coherence='c_v')\n",
    "        coherence_score = coherence_model.get_coherence()\n",
    "        self.assertGreater(coherence_score, 0.0)  # Adjust threshold as needed\n",
    "\n",
    "    def test_topic_diversity(self):\n",
    "        N = 20\n",
    "        topic_words = [word for topic in self.lda_model.show_topics(num_topics=-1, num_words=N, formatted=False)\n",
    "                       for word, _ in topic[1]]\n",
    "        diversity_score = len(set(topic_words)) / \\\n",
    "            (self.lda_model.num_topics * N)\n",
    "        self.assertGreater(diversity_score, 0.0)  # Adjust threshold as needed\n",
    "\n",
    "    def test_new_text_assignment(self):\n",
    "        new_text = \"This is a sample text about artificial intelligence and machine learning.\"\n",
    "        processed_text = self.preprocess_text(new_text)\n",
    "        bow = self.dictionary.doc2bow(processed_text)\n",
    "        topics = self.lda_model.get_document_topics(bow)\n",
    "        self.assertTrue(len(topics) > 0)\n",
    "        self.assertIsInstance(topics[0][0], int)\n",
    "        self.assertIsInstance(topics[0][1], float)\n",
    "        self.assertTrue(0 <= topics[0][1] <= 1)\n",
    "\n",
    "    def test_model_consistency(self):\n",
    "        text = \"This is a test text for consistency.\"\n",
    "        processed_text = self.preprocess_text(text)\n",
    "        bow = self.dictionary.doc2bow(processed_text)\n",
    "        topics1 = self.lda_model.get_document_topics(bow)\n",
    "        topics2 = self.lda_model.get_document_topics(bow)\n",
    "        self.assertEqual(topics1, topics2)\n",
    "\n",
    "    def test_preprocessing_function(self):\n",
    "        text = \"This is a TEST sentence with UPPERCASE words and punctuation!\"\n",
    "        processed = self.preprocess_text(text)\n",
    "        self.assertTrue(all(word.islower() for word in processed))\n",
    "        self.assertTrue(all(word.isalpha() for word in processed))\n",
    "\n",
    "    def test_dictionary_filter(self):\n",
    "        # Check if extreme words have been filtered out\n",
    "        self.assertTrue(\n",
    "            all(self.dictionary.dfs[id] >= 5 for id in self.dictionary.dfs))\n",
    "        self.assertTrue(all(\n",
    "            self.dictionary.dfs[id] / len(self.corpus) <= 0.5 for id in self.dictionary.dfs))\n",
    "\n",
    "    def test_corpus_format(self):\n",
    "        self.assertTrue(all(isinstance(doc, list)\n",
    "                        for doc in self.corpus[:5]))  # Test with first 5 documents\n",
    "        self.assertTrue(all(isinstance(word_id, int) and isinstance(count, int)\n",
    "                            for doc in self.corpus[:5] for word_id, count in doc))\n",
    "\n",
    "\n",
    "unittest.main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
