{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intent evaluator: Finetuned VS Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score\n",
    ")\n",
    "from transformers import (\n",
    "    Pipeline,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TextClassificationPipeline\n",
    ")\n",
    "from datasets import Dataset\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    \"\"\"Comprehensive evaluation of pre-trained vs fine-tuned models.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        default_model_name: str,\n",
    "        finetuned_model_path: str,\n",
    "        task_type: str = \"sentiment\",\n",
    "        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    ):\n",
    "        \"\"\"Initialize model evaluator.\n",
    "\n",
    "        Args:\n",
    "            default_model_name: HuggingFace model name/path for default model\n",
    "            finetuned_model_path: Path to saved fine-tuned model\n",
    "            task_type: One of [\"sentiment\", \"intent\", \"topic\"]\n",
    "            device: Device to run models on\n",
    "        \"\"\"\n",
    "        self.task_type = task_type\n",
    "        self.device = device\n",
    "\n",
    "        # Load both models\n",
    "        self.default_tokenizer = AutoTokenizer.from_pretrained(\n",
    "            default_model_name)\n",
    "        self.default_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            default_model_name\n",
    "        ).to(device)\n",
    "\n",
    "        self.finetuned_tokenizer = AutoTokenizer.from_pretrained(\n",
    "            finetuned_model_path)\n",
    "        self.finetuned_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            finetuned_model_path\n",
    "        ).to(device)\n",
    "\n",
    "        # Create pipelines\n",
    "        self.default_pipeline = TextClassificationPipeline(\n",
    "            model=self.default_model,\n",
    "            tokenizer=self.default_tokenizer,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        self.finetuned_pipeline = TextClassificationPipeline(\n",
    "            model=self.finetuned_model,\n",
    "            tokenizer=self.finetuned_tokenizer,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        self.label2id = self.default_model.config.label2id\n",
    "        self.id2label = self.default_model.config.id2label\n",
    "\n",
    "    def get_predictions(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        pipeline: Pipeline\n",
    "    ) -> Tuple[List[int], List[float]]:\n",
    "        \"\"\"Get predictions and confidence scores from a pipeline.\"\"\"\n",
    "        results = pipeline(texts)\n",
    "        predictions = []\n",
    "        confidences = []\n",
    "\n",
    "        for result in results:\n",
    "            label = result['label']\n",
    "            if isinstance(label, str):\n",
    "                label_id = self.label2id.get(label, -1)\n",
    "            else:\n",
    "                label_id = label\n",
    "            predictions.append(label_id)\n",
    "            confidences.append(result['score'])\n",
    "\n",
    "        return predictions, confidences\n",
    "\n",
    "    def evaluate_on_dataset(\n",
    "        self,\n",
    "        test_dataset: Dataset,\n",
    "        text_column: str = \"text\",\n",
    "        label_column: str = \"label\"\n",
    "    ) -> Dict:\n",
    "        \"\"\"Evaluate both models on a test dataset.\"\"\"\n",
    "        texts = test_dataset[text_column]\n",
    "        true_labels = test_dataset[label_column]\n",
    "\n",
    "        # Get predictions from both models\n",
    "        default_preds, default_conf = self.get_predictions(\n",
    "            texts, self.default_pipeline)\n",
    "        finetuned_preds, finetuned_conf = self.get_predictions(\n",
    "            texts, self.finetuned_pipeline)\n",
    "\n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'default': self._calculate_metrics(true_labels, default_preds, default_conf),\n",
    "            'finetuned': self._calculate_metrics(true_labels, finetuned_preds, finetuned_conf)\n",
    "        }\n",
    "\n",
    "        # Perform statistical significance tests\n",
    "        metrics['statistical_tests'] = self._perform_statistical_tests(\n",
    "            true_labels, default_preds, finetuned_preds\n",
    "        )\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def _calculate_metrics(\n",
    "        self,\n",
    "        true_labels: List[int],\n",
    "        pred_labels: List[int],\n",
    "        confidences: List[float]\n",
    "    ) -> Dict:\n",
    "        \"\"\"Calculate comprehensive metrics for model evaluation.\"\"\"\n",
    "        metrics = {}\n",
    "\n",
    "        # Basic classification metrics\n",
    "        metrics['classification_report'] = classification_report(\n",
    "            true_labels, pred_labels, output_dict=True\n",
    "        )\n",
    "        metrics['confusion_matrix'] = confusion_matrix(\n",
    "            true_labels, pred_labels)\n",
    "\n",
    "        # Calculate ROC and PR curves for each class\n",
    "        metrics['roc_curves'] = {}\n",
    "        metrics['pr_curves'] = {}\n",
    "\n",
    "        n_classes = len(self.label2id)\n",
    "        for i in range(n_classes):\n",
    "            # Convert to binary problem for each class\n",
    "            binary_true = [1 if label == i else 0 for label in true_labels]\n",
    "            binary_conf = [conf if pred == i else 1 -\n",
    "                           conf for pred, conf in zip(pred_labels, confidences)]\n",
    "\n",
    "            # ROC curve\n",
    "            fpr, tpr, _ = roc_curve(binary_true, binary_conf)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            metrics['roc_curves'][self.id2label[i]] = {\n",
    "                'fpr': fpr.tolist(),\n",
    "                'tpr': tpr.tolist(),\n",
    "                'auc': roc_auc\n",
    "            }\n",
    "\n",
    "            # PR curve\n",
    "            precision, recall, _ = precision_recall_curve(\n",
    "                binary_true, binary_conf)\n",
    "            avg_precision = average_precision_score(binary_true, binary_conf)\n",
    "            metrics['pr_curves'][self.id2label[i]] = {\n",
    "                'precision': precision.tolist(),\n",
    "                'recall': recall.tolist(),\n",
    "                'avg_precision': avg_precision\n",
    "            }\n",
    "\n",
    "        # Confidence analysis\n",
    "        metrics['confidence_stats'] = {\n",
    "            'mean': np.mean(confidences),\n",
    "            'std': np.std(confidences),\n",
    "            'median': np.median(confidences),\n",
    "            'correct_conf': np.mean([conf for pred, conf, true in zip(pred_labels, confidences, true_labels) if pred == true]),\n",
    "            'incorrect_conf': np.mean([conf for pred, conf, true in zip(pred_labels, confidences, true_labels) if pred != true])\n",
    "        }\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def _perform_statistical_tests(\n",
    "        self,\n",
    "        true_labels: List[int],\n",
    "        default_preds: List[int],\n",
    "        finetuned_preds: List[int]\n",
    "    ) -> Dict:\n",
    "        \"\"\"Perform statistical significance tests between models.\"\"\"\n",
    "        # McNemar's test for paired nominal data\n",
    "        default_correct = [1 if d == t else 0 for d,\n",
    "                           t in zip(default_preds, true_labels)]\n",
    "        finetuned_correct = [1 if f == t else 0 for f,\n",
    "                             t in zip(finetuned_preds, true_labels)]\n",
    "\n",
    "        contingency_table = np.zeros((2, 2))\n",
    "        for d, f in zip(default_correct, finetuned_correct):\n",
    "            contingency_table[d][f] += 1\n",
    "\n",
    "        mcnemar_statistic, mcnemar_p_value = stats.mcnemar(contingency_table)\n",
    "\n",
    "        return {\n",
    "            'mcnemar_test': {\n",
    "                'statistic': float(mcnemar_statistic),\n",
    "                'p_value': float(mcnemar_p_value)\n",
    "            },\n",
    "            'contingency_table': contingency_table.tolist()\n",
    "        }\n",
    "\n",
    "    def plot_evaluation_results(self, metrics: Dict, save_path: str = None):\n",
    "        \"\"\"Create comprehensive visualization of evaluation results.\"\"\"\n",
    "        fig = plt.figure(figsize=(20, 15))\n",
    "\n",
    "        # 1. Confusion Matrices\n",
    "        plt.subplot(2, 3, 1)\n",
    "        self._plot_confusion_matrix(\n",
    "            metrics['default']['confusion_matrix'],\n",
    "            'Default Model Confusion Matrix'\n",
    "        )\n",
    "\n",
    "        plt.subplot(2, 3, 2)\n",
    "        self._plot_confusion_matrix(\n",
    "            metrics['finetuned']['confusion_matrix'],\n",
    "            'Fine-tuned Model Confusion Matrix'\n",
    "        )\n",
    "\n",
    "        # 2. ROC Curves\n",
    "        plt.subplot(2, 3, 3)\n",
    "        self._plot_roc_curves(\n",
    "            metrics['default']['roc_curves'],\n",
    "            metrics['finetuned']['roc_curves']\n",
    "        )\n",
    "\n",
    "        # 3. PR Curves\n",
    "        plt.subplot(2, 3, 4)\n",
    "        self._plot_pr_curves(\n",
    "            metrics['default']['pr_curves'],\n",
    "            metrics['finetuned']['pr_curves']\n",
    "        )\n",
    "\n",
    "        # 4. Confidence Distribution\n",
    "        plt.subplot(2, 3, 5)\n",
    "        self._plot_confidence_distribution(\n",
    "            metrics['default']['confidence_stats'],\n",
    "            metrics['finetuned']['confidence_stats']\n",
    "        )\n",
    "\n",
    "        # 5. Performance Comparison\n",
    "        plt.subplot(2, 3, 6)\n",
    "        self._plot_performance_comparison(\n",
    "            metrics['default']['classification_report'],\n",
    "            metrics['finetuned']['classification_report']\n",
    "        )\n",
    "\n",
    "        plt.tight_layout()\n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "        plt.show()\n",
    "\n",
    "    def _plot_confusion_matrix(self, cm: np.ndarray, title: str):\n",
    "        \"\"\"Plot a confusion matrix.\"\"\"\n",
    "        sns.heatmap(\n",
    "            cm,\n",
    "            annot=True,\n",
    "            fmt='d',\n",
    "            cmap='Blues',\n",
    "            xticklabels=list(self.id2label.values()),\n",
    "            yticklabels=list(self.id2label.values())\n",
    "        )\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "\n",
    "    def _plot_roc_curves(self, default_curves: Dict, finetuned_curves: Dict):\n",
    "        \"\"\"Plot ROC curves for all classes.\"\"\"\n",
    "        for label in self.id2label.values():\n",
    "            # Default model\n",
    "            plt.plot(\n",
    "                default_curves[label]['fpr'],\n",
    "                default_curves[label]['tpr'],\n",
    "                '--',\n",
    "                label=f'Default - {label} (AUC = {default_curves[label][\"auc\"]:.2f})'\n",
    "            )\n",
    "            # Fine-tuned model\n",
    "            plt.plot(\n",
    "                finetuned_curves[label]['fpr'],\n",
    "                finetuned_curves[label]['tpr'],\n",
    "                '-',\n",
    "                label=f'Fine-tuned - {label} (AUC = {finetuned_curves[label][\"auc\"]:.2f})'\n",
    "            )\n",
    "\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curves Comparison')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "\n",
    "    def _plot_pr_curves(self, default_curves: Dict, finetuned_curves: Dict):\n",
    "        \"\"\"Plot Precision-Recall curves for all classes.\"\"\"\n",
    "        for label in self.id2label.values():\n",
    "            # Default model\n",
    "            plt.plot(\n",
    "                default_curves[label]['recall'],\n",
    "                default_curves[label]['precision'],\n",
    "                '--',\n",
    "                label=f'Default - {label} (AP = {default_curves[label][\"avg_precision\"]:.2f})'\n",
    "            )\n",
    "            # Fine-tuned model\n",
    "            plt.plot(\n",
    "                finetuned_curves[label]['recall'],\n",
    "                finetuned_curves[label]['precision'],\n",
    "                '-',\n",
    "                label=f'Fine-tuned - {label} (AP = {finetuned_curves[label][\"avg_precision\"]:.2f})'\n",
    "            )\n",
    "\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Precision-Recall Curves Comparison')\n",
    "        plt.legend(loc=\"lower left\")\n",
    "\n",
    "    def _plot_confidence_distribution(\n",
    "        self,\n",
    "        default_stats: Dict,\n",
    "        finetuned_stats: Dict\n",
    "    ):\n",
    "        \"\"\"Plot confidence score distributions.\"\"\"\n",
    "        stats = pd.DataFrame({\n",
    "            'Model': ['Default', 'Fine-tuned'] * 2,\n",
    "            'Type': ['Correct', 'Correct', 'Incorrect', 'Incorrect'],\n",
    "            'Confidence': [\n",
    "                default_stats['correct_conf'],\n",
    "                finetuned_stats['correct_conf'],\n",
    "                default_stats['incorrect_conf'],\n",
    "                finetuned_stats['incorrect_conf']\n",
    "            ]\n",
    "        })\n",
    "\n",
    "        sns.barplot(x='Model', y='Confidence', hue='Type', data=stats)\n",
    "        plt.title('Confidence Score Distribution')\n",
    "\n",
    "    def _plot_performance_comparison(\n",
    "        self,\n",
    "        default_report: Dict,\n",
    "        finetuned_report: Dict\n",
    "    ):\n",
    "        \"\"\"Plot performance metrics comparison.\"\"\"\n",
    "        metrics = ['precision', 'recall', 'f1-score']\n",
    "        labels = list(self.id2label.values()) + ['macro avg']\n",
    "\n",
    "        comparison_data = []\n",
    "        for label in labels:\n",
    "            for metric in metrics:\n",
    "                comparison_data.append({\n",
    "                    'Label': label,\n",
    "                    'Metric': metric,\n",
    "                    'Default': default_report[label][metric],\n",
    "                    'Fine-tuned': finetuned_report[label][metric]\n",
    "                })\n",
    "\n",
    "        df = pd.DataFrame(comparison_data)\n",
    "        df_melted = df.melt(\n",
    "            id_vars=['Label', 'Metric'],\n",
    "            var_name='Model',\n",
    "            value_name='Score'\n",
    "        )\n",
    "\n",
    "        sns.barplot(\n",
    "            data=df_melted,\n",
    "            x='Label',\n",
    "            y='Score',\n",
    "            hue='Model',\n",
    "            palette=['lightblue', 'darkblue'],\n",
    "            alpha=0.6\n",
    "        )\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.title('Performance Metrics Comparison')\n",
    "        plt.tight_layout()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # For sentiment analysis\n",
    "    evaluator = ModelEvaluator(\n",
    "        default_model_name=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "        finetuned_model_path=\"path/to/your/finetuned/sentiment/model\",\n",
    "        task_type=\"sentiment\"\n",
    "    )\n",
    "\n",
    "    # Load your test dataset\n",
    "    from datasets import load_dataset\n",
    "    test_dataset = load_dataset(\"your_test_dataset\")[\"test\"]\n",
    "\n",
    "    # Run evaluation\n",
    "    metrics = evaluator.evaluate_on_dataset(\n",
    "        test_dataset,\n",
    "        text_column=\"text\",\n",
    "        label_column=\"label\"\n",
    "    )\n",
    "\n",
    "    # Plot results\n",
    "    evaluator.plot_evaluation_results(\n",
    "        metrics, save_path=\"evaluation_results.png\")\n",
    "\n",
    "    # Print statistical significance\n",
    "    print(\"\\nStatistical Significance Tests:\")\n",
    "    print(\n",
    "        f\"McNemar's test p-value: {metrics['statistical_tests']['mcnemar_test']['p_value']}\")\n",
    "    if metrics['statistical_tests']['mcnemar_test']['p_value'] < 0.05:\n",
    "        print(\"The difference between models is statistically significant\")\n",
    "    else:\n",
    "        print(\"The difference between models is not statistically significant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intent Recognition & Topic Model Evaluators\n",
    "class IntentRecognitionEvaluator(ModelEvaluator):\n",
    "    \"\"\"Specialized evaluator for intent recognition models.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def evaluate_on_dataset(self, test_dataset: Dataset, *args, **kwargs):\n",
    "        metrics = super().evaluate_on_dataset(test_dataset, *args, **kwargs)\n",
    "\n",
    "        # Add intent-specific metrics\n",
    "        for model_type in ['default', 'finetuned']:\n",
    "            # Add intent confusion analysis\n",
    "            metrics[model_type]['intent_confusion'] = self._analyze_intent_confusion(\n",
    "                test_dataset['label'],\n",
    "                metrics[model_type]['predictions']\n",
    "            )\n",
    "\n",
    "            # Add intent transition analysis\n",
    "            metrics[model_type]['intent_transitions'] = self._analyze_intent_transitions(\n",
    "                test_dataset['label'],\n",
    "                metrics[model_type]['predictions']\n",
    "            )\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def _analyze_intent_confusion(self, true_intents, pred_intents):\n",
    "        \"\"\"Analyze which intents are most commonly confused.\"\"\"\n",
    "        confusion_pairs = []\n",
    "        for true, pred in zip(true_intents, pred_intents):\n",
    "            if true != pred:\n",
    "                confusion_pairs.append(\n",
    "                    (self.id2label[true], self.id2label[pred]))\n",
    "\n",
    "        confusion_counts = pd.Series(confusion_pairs).value_counts()\n",
    "        return confusion_counts.to_dict()\n",
    "\n",
    "    def _analyze_intent_transitions(self, true_intents, pred_intents):\n",
    "        \"\"\"Analyze intent prediction transitions in conversation flow.\"\"\"\n",
    "        transitions = []\n",
    "        for i in range(len(true_intents) - 1):\n",
    "            true_transition = (self.id2label[true_intents[i]],\n",
    "                               self.id2label[true_intents[i + 1]])\n",
    "            pred_transition = (self.id2label[pred_intents[i]],\n",
    "                               self.id2label[pred_intents[i + 1]])\n",
    "            transitions.append((true_transition, pred_transition))\n",
    "\n",
    "        return pd.Series(transitions).value_counts().to_dict()\n",
    "\n",
    "\n",
    "class TopicModelEvaluator(ModelEvaluator):\n",
    "    \"\"\"Specialized evaluator for topic classification models.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def evaluate_on_dataset(self, test_dataset: Dataset, *args, **kwargs):\n",
    "        metrics = super().evaluate_on_dataset(test_dataset, *args, **kwargs)\n",
    "\n",
    "        # Add topic-specific metrics\n",
    "        for model_type in ['default', 'finetuned']:\n",
    "            # Add topic coherence analysis\n",
    "            metrics[model_type]['topic_coherence'] = self._analyze_topic_coherence(\n",
    "                test_dataset['text'],\n",
    "                metrics[model_type]['predictions']\n",
    "            )\n",
    "\n",
    "            # Add topic diversity analysis\n",
    "            metrics[model_type]['topic_diversity'] = self._analyze_topic_diversity(\n",
    "                test_dataset['text'],\n",
    "                metrics[model_type]['predictions']\n",
    "            )\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def _analyze_topic_coherence(self, texts, topic_assignments):\n",
    "        \"\"\"Analyze semantic coherence of texts within each topic.\"\"\"\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "        coherence_scores = {}\n",
    "\n",
    "        for topic_id in self.id2label:\n",
    "            # Get texts for this topic\n",
    "            topic_texts = [text for text, topic in zip(texts, topic_assignments)\n",
    "                           if topic == topic_id]\n",
    "\n",
    "            if not topic_texts:\n",
    "                continue\n",
    "\n",
    "            # Calculate TF-IDF vectors\n",
    "            vectorizer = TfidfVectorizer(stop_words='english')\n",
    "            tfidf_matrix = vectorizer.fit_transform(topic_texts)\n",
    "\n",
    "            # Calculate average pairwise similarity\n",
    "            similarities = cosine_similarity(tfidf_matrix)\n",
    "            coherence = np.mean(\n",
    "                similarities[np.triu_indices(similarities.shape[0], k=1)])\n",
    "\n",
    "            coherence_scores[self.id2label[topic_id]] = float(coherence)\n",
    "\n",
    "        return coherence_scores\n",
    "\n",
    "    def _analyze_topic_diversity(self, texts, topic_assignments):\n",
    "        \"\"\"Analyze diversity of topics and their distributions.\"\"\"\n",
    "        from collections import Counter\n",
    "\n",
    "        # Topic distribution\n",
    "        topic_dist = Counter(topic_assignments)\n",
    "\n",
    "        # Calculate entropy of distribution\n",
    "        total = sum(topic_dist.values())\n",
    "        probs = [count/total for count in topic_dist.values()]\n",
    "        entropy = -sum(p * np.log(p) for p in probs if p > 0)\n",
    "\n",
    "        return {\n",
    "            'topic_distribution': {self.id2label[k]: v for k, v in topic_dist.items()},\n",
    "            'topic_entropy': float(entropy)\n",
    "        }\n",
    "\n",
    "\n",
    "# Example usage script showing how to evaluate all three model types\n",
    "def evaluate_all_models(test_data_paths: Dict[str, str], model_paths: Dict[str, Dict[str, str]]):\n",
    "    \"\"\"\n",
    "    Evaluate sentiment, intent, and topic models.\n",
    "\n",
    "    Args:\n",
    "        test_data_paths: Dict with paths to test datasets for each task\n",
    "        model_paths: Dict with default and finetuned model paths for each task\n",
    "    \"\"\"\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # 1. Sentiment Analysis\n",
    "    print(\"Evaluating Sentiment Analysis Models...\")\n",
    "    sentiment_evaluator = ModelEvaluator(\n",
    "        default_model_name=model_paths['sentiment']['default'],\n",
    "        finetuned_model_path=model_paths['sentiment']['finetuned'],\n",
    "        task_type=\"sentiment\"\n",
    "    )\n",
    "\n",
    "    sentiment_test_data = load_dataset(\n",
    "        \"csv\", data_files=test_data_paths['sentiment'])\n",
    "    results['sentiment'] = sentiment_evaluator.evaluate_on_dataset(\n",
    "        sentiment_test_data['train']\n",
    "    )\n",
    "    sentiment_evaluator.plot_evaluation_results(\n",
    "        results['sentiment'],\n",
    "        save_path=\"sentiment_evaluation.png\"\n",
    "    )\n",
    "\n",
    "    # 2. Intent Recognition\n",
    "    print(\"\\nEvaluating Intent Recognition Models...\")\n",
    "    intent_evaluator = IntentRecognitionEvaluator(\n",
    "        default_model_name=model_paths['intent']['default'],\n",
    "        finetuned_model_path=model_paths['intent']['finetuned'],\n",
    "        task_type=\"intent\"\n",
    "    )\n",
    "\n",
    "    intent_test_data = load_dataset(\n",
    "        \"csv\", data_files=test_data_paths['intent'])\n",
    "    results['intent'] = intent_evaluator.evaluate_on_dataset(\n",
    "        intent_test_data['train']\n",
    "    )\n",
    "    intent_evaluator.plot_evaluation_results(\n",
    "        results['intent'],\n",
    "        save_path=\"intent_evaluation.png\"\n",
    "    )\n",
    "\n",
    "    # 3. Topic Classification\n",
    "    print(\"\\nEvaluating Topic Classification Models...\")\n",
    "    topic_evaluator = TopicModelEvaluator(\n",
    "        default_model_name=model_paths['topic']['default'],\n",
    "        finetuned_model_path=model_paths['topic']['finetuned'],\n",
    "        task_type=\"topic\"\n",
    "    )\n",
    "\n",
    "    topic_test_data = load_dataset(\"csv\", data_files=test_data_paths['topic'])\n",
    "    results['topic'] = topic_evaluator.evaluate_on_dataset(\n",
    "        topic_test_data['train']\n",
    "    )\n",
    "    topic_evaluator.plot_evaluation_results(\n",
    "        results['topic'],\n",
    "        save_path=\"topic_evaluation.png\"\n",
    "    )\n",
    "\n",
    "    # Print summary of improvements\n",
    "    print(\"\\nSummary of Improvements:\")\n",
    "    for task in ['sentiment', 'intent', 'topic']:\n",
    "        print(f\"\\n{task.upper()} ANALYSIS:\")\n",
    "        default_f1 = results[task]['default']['classification_report']['macro avg']['f1-score']\n",
    "        finetuned_f1 = results[task]['finetuned']['classification_report']['macro avg']['f1-score']\n",
    "        improvement = ((finetuned_f1 - default_f1) / default_f1) * 100\n",
    "\n",
    "        print(f\"Default Model F1: {default_f1:.3f}\")\n",
    "        print(f\"Fine-tuned Model F1: {finetuned_f1:.3f}\")\n",
    "        print(f\"Improvement: {improvement:.1f}%\")\n",
    "\n",
    "        # Print statistical significance\n",
    "        p_value = results[task]['statistical_tests']['mcnemar_test']['p_value']\n",
    "        print(f\"Statistical Significance (p-value): {p_value:.4f}\")\n",
    "\n",
    "        if task == 'intent':\n",
    "            print(\"\\nMost Common Intent Confusions (Fine-tuned model):\")\n",
    "            for (true_intent, pred_intent), count in list(results[task]['finetuned']['intent_confusion'].items())[:5]:\n",
    "                print(f\"{true_intent} → {pred_intent}: {count} times\")\n",
    "\n",
    "        elif task == 'topic':\n",
    "            print(\"\\nTopic Coherence Scores (Fine-tuned model):\")\n",
    "            coherence_scores = results[task]['finetuned']['topic_coherence']\n",
    "            for topic, score in sorted(coherence_scores.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "                print(f\"{topic}: {score:.3f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    test_data_paths = {\n",
    "        'sentiment': 'path/to/sentiment_test.csv',\n",
    "        'intent': 'path/to/intent_test.csv',\n",
    "        'topic': 'path/to/topic_test.csv'\n",
    "    }\n",
    "\n",
    "    model_paths = {\n",
    "        'sentiment': {\n",
    "            'default': 'distilbert-base-uncased-finetuned-sst-2-english',\n",
    "            'finetuned': 'path/to/finetuned/sentiment/model'\n",
    "        },\n",
    "        'intent': {\n",
    "            'default': 'Falconsai/intent_classification',\n",
    "            'finetuned': 'path/to/finetuned/intent/model'\n",
    "        },\n",
    "        'topic': {\n",
    "            'default': 'dstefa/roberta-base_topic_classification_nyt_news',\n",
    "            'finetuned': 'path/to/finetuned/topic/model'\n",
    "        }\n",
    "    }\n",
    "\n",
    "    results = evaluate_all_models(test_data_paths, model_paths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
