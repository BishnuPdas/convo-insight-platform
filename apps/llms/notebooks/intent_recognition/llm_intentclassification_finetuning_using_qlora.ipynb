{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import json\n",
    "\n",
    "\n",
    "class IntentClassifier:\n",
    "    def __init__(self, model_name=\"microsoft/deberta-v2-xlarge\", output_dir=\"intent_model\"):\n",
    "        self.model_name = model_name\n",
    "        self.output_dir = output_dir\n",
    "        self.label_encoder = LabelEncoder()\n",
    "\n",
    "    def prepare_intent_dataset(self, data_path):\n",
    "        \"\"\"\n",
    "        Prepare dataset from a CSV file containing 'text' and 'intent' columns\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(data_path)\n",
    "\n",
    "        # Encode intent labels\n",
    "        self.label_encoder.fit(df['intent'])\n",
    "        df['label'] = self.label_encoder.transform(df['intent'])\n",
    "\n",
    "        # Save label encoder classes\n",
    "        self.num_labels = len(self.label_encoder.classes_)\n",
    "        with open(os.path.join(self.output_dir, 'intent_labels.json'), 'w') as f:\n",
    "            json.dump(\n",
    "                {\n",
    "                    'labels': self.label_encoder.classes_.tolist(),\n",
    "                    'label_to_id': {l: i for i, l in enumerate(self.label_encoder.classes_)}\n",
    "                },\n",
    "                f\n",
    "            )\n",
    "\n",
    "        # Split dataset\n",
    "        train_df, temp_df = train_test_split(\n",
    "            df, test_size=0.3, random_state=42, stratify=df['label'])\n",
    "        val_df, test_df = train_test_split(\n",
    "            temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'])\n",
    "\n",
    "        # Convert to HuggingFace datasets\n",
    "        train_dataset = Dataset.from_pandas(train_df)\n",
    "        val_dataset = Dataset.from_pandas(val_df)\n",
    "        test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "        return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "    def setup_intent_model(self):\n",
    "        \"\"\"\n",
    "        Setup model with QLoRA configuration for intent classification\n",
    "        \"\"\"\n",
    "        # Quantization configuration\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=False\n",
    "        )\n",
    "\n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "\n",
    "        # Load model\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            num_labels=self.num_labels,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "\n",
    "        # Prepare model for k-bit training\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "        # LoRA configuration\n",
    "        lora_config = LoraConfig(\n",
    "            r=16,                     # Rank dimension\n",
    "            lora_alpha=32,           # Alpha parameter for scaling\n",
    "            target_modules=[\"query\", \"key\", \"value\"],\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=\"SEQ_CLS\"\n",
    "        )\n",
    "\n",
    "        # Create PEFT model\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        model.print_trainable_parameters()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def preprocess_function(self, examples):\n",
    "        \"\"\"\n",
    "        Tokenize and prepare inputs\n",
    "        \"\"\"\n",
    "        return self.tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "    def compute_metrics(self, eval_pred):\n",
    "        \"\"\"\n",
    "        Compute metrics for intent classification evaluation\n",
    "        \"\"\"\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "        # Get classification report\n",
    "        report = classification_report(\n",
    "            labels,\n",
    "            predictions,\n",
    "            target_names=self.label_encoder.classes_,\n",
    "            output_dict=True\n",
    "        )\n",
    "\n",
    "        # Extract metrics\n",
    "        metrics = {\n",
    "            'accuracy': report['accuracy'],\n",
    "            'macro_f1': report['macro avg']['f1-score'],\n",
    "            'weighted_f1': report['weighted avg']['f1-score']\n",
    "        }\n",
    "\n",
    "        # Add per-intent f1 scores\n",
    "        for intent in self.label_encoder.classes_:\n",
    "            metrics[f'f1_{intent}'] = report[intent]['f1-score']\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def setup_training_args(self):\n",
    "        \"\"\"\n",
    "        Setup training arguments optimized for intent classification\n",
    "        \"\"\"\n",
    "        return TrainingArguments(\n",
    "            output_dir=self.output_dir,\n",
    "            learning_rate=2e-4,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=16,\n",
    "            num_train_epochs=10,\n",
    "            weight_decay=0.01,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=50,\n",
    "            save_strategy=\"steps\",\n",
    "            save_steps=50,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"weighted_f1\",\n",
    "            push_to_hub=False,\n",
    "            gradient_accumulation_steps=2,\n",
    "            fp16=True,\n",
    "            logging_steps=10,\n",
    "            warmup_ratio=0.1,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            report_to=\"tensorboard\"\n",
    "        )\n",
    "\n",
    "    def train(self, data_path):\n",
    "        \"\"\"\n",
    "        Main training function\n",
    "        \"\"\"\n",
    "        # Create output directory\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "        # Prepare datasets\n",
    "        train_dataset, val_dataset, test_dataset = self.prepare_intent_dataset(\n",
    "            data_path)\n",
    "\n",
    "        # Setup model\n",
    "        model = self.setup_intent_model()\n",
    "\n",
    "        # Preprocess datasets\n",
    "        train_dataset = train_dataset.map(\n",
    "            self.preprocess_function,\n",
    "            batched=True,\n",
    "            remove_columns=train_dataset.column_names\n",
    "        )\n",
    "        val_dataset = val_dataset.map(\n",
    "            self.preprocess_function,\n",
    "            batched=True,\n",
    "            remove_columns=val_dataset.column_names\n",
    "        )\n",
    "        test_dataset = test_dataset.map(\n",
    "            self.preprocess_function,\n",
    "            batched=True,\n",
    "            remove_columns=test_dataset.column_names\n",
    "        )\n",
    "\n",
    "        # Setup training arguments\n",
    "        training_args = self.setup_training_args()\n",
    "\n",
    "        # Initialize trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            compute_metrics=self.compute_metrics,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "        )\n",
    "\n",
    "        # Train model\n",
    "        trainer.train()\n",
    "\n",
    "        # Evaluate on test set\n",
    "        test_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "        # Save results\n",
    "        with open(os.path.join(self.output_dir, 'test_results.json'), 'w') as f:\n",
    "            json.dump(test_results, f, indent=4)\n",
    "\n",
    "        # Save model and tokenizer\n",
    "        trainer.save_model(self.output_dir)\n",
    "        self.tokenizer.save_pretrained(self.output_dir)\n",
    "\n",
    "        return test_results\n",
    "\n",
    "    def predict_intent(self, text):\n",
    "        \"\"\"\n",
    "        Predict intent for new text\n",
    "        \"\"\"\n",
    "        # Load the saved model and tokenizer\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.output_dir)\n",
    "\n",
    "        # Tokenize input\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "\n",
    "        # Get prediction\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            predictions = torch.softmax(outputs.logits, dim=1)\n",
    "            predicted_class = torch.argmax(predictions, dim=1).item()\n",
    "\n",
    "        # Get intent label and confidence\n",
    "        predicted_intent = self.label_encoder.inverse_transform([predicted_class])[\n",
    "            0]\n",
    "        confidence = predictions[0][predicted_class].item()\n",
    "\n",
    "        # Get probabilities for all intents\n",
    "        intent_probabilities = {\n",
    "            intent: predictions[0][i].item()\n",
    "            for i, intent in enumerate(self.label_encoder.classes_)\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            'intent': predicted_intent,\n",
    "            'confidence': confidence,\n",
    "            'probabilities': intent_probabilities\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Set paths\n",
    "    # CSV with 'text' and 'intent' columns\n",
    "    DATA_PATH = \"path/to/your/intent_dataset.csv\"\n",
    "    OUTPUT_DIR = \"intent_classifier_output\"\n",
    "\n",
    "    # Initialize and train classifier\n",
    "    classifier = IntentClassifier(output_dir=OUTPUT_DIR)\n",
    "    test_results = classifier.train(DATA_PATH)\n",
    "    print(f\"Test results: {test_results}\")\n",
    "\n",
    "    # Example prediction\n",
    "    test_text = \"What is the weather like today?\"\n",
    "    result = classifier.predict_intent(test_text)\n",
    "    print(f\"Predicted intent: {result}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
